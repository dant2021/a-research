{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch wandb datasets tqdm huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login with your token\n",
    "login(\"your token\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"your token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "class LatentThoughtLLM(nn.Module):\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-3.2-1B-Instruct\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.rms_norm = nn.LayerNorm(self.hidden_size, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, prompt_mask=None, alpha_schedule=0.0):\n",
    "        # First pass to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        # Get base embeddings and hidden states\n",
    "        token_embeddings = self.model.get_input_embeddings()(input_ids)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Normalize hidden states\n",
    "        normalized_hidden = self.rms_norm(last_hidden)\n",
    "        scale_factor = token_embeddings.norm(dim=-1).mean() / normalized_hidden.norm(dim=-1).mean()\n",
    "        normalized_hidden = normalized_hidden * scale_factor\n",
    "        \n",
    "        # Shift hidden states left by one position\n",
    "        normalized_hidden = torch.cat([normalized_hidden[:, 1:], normalized_hidden[:, -1:]], dim=1)\n",
    "\n",
    "        mixed_embeddings = token_embeddings.clone()\n",
    "        is_reasoning_token = prompt_mask.bool() & attention_mask.bool() \n",
    "        \n",
    "        # Mix embeddings only for reasoning tokens\n",
    "        mixed_embeddings[is_reasoning_token] = (\n",
    "            alpha_schedule * normalized_hidden[is_reasoning_token] + \n",
    "            (1 - alpha_schedule) * token_embeddings[is_reasoning_token]\n",
    "        )\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"\\nBefore mixing:\")\n",
    "        print(f\"First hidden norm: {token_embeddings.norm(dim=-1).mean():.3f} std: {token_embeddings.std(dim=-1).mean():.3f}\")\n",
    "        print(f\"Normalized hidden states norm: {normalized_hidden.norm(dim=-1).mean():.3f} std: {normalized_hidden.std(dim=-1).mean():.3f}\")\n",
    "        print(f\"Alpha: {alpha_schedule:.3f}\")\n",
    "        print(f\"Number of tokens to mix: {is_reasoning_token.bool().sum().item()}\")\n",
    "        print(f\"Mixed embeddings norm: {mixed_embeddings[is_reasoning_token].norm(dim=-1).mean():.3f} std: {mixed_embeddings[is_reasoning_token].std(dim=-1).mean():.3f}\")\n",
    "\n",
    "        # Forward through model again with mixed embeddings\n",
    "        final_outputs = self.model(\n",
    "            inputs_embeds=mixed_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        final_hidden = final_outputs.hidden_states[-1]\n",
    "        \n",
    "        return final_outputs, last_hidden, final_hidden, is_reasoning_token\n",
    "\n",
    "\n",
    "\n",
    "def LatentThoughtLoss(outputs, hidden_states, target_embeddings, is_reasoning_token):\n",
    "    # Standard LM loss from outputs, only for reasoning tokens\n",
    "    ce_loss = outputs.loss\n",
    "    \n",
    "    # Embedding matching loss only for reasoning tokens\n",
    "    mse_loss = F.mse_loss(\n",
    "        hidden_states[is_reasoning_token],\n",
    "        target_embeddings[is_reasoning_token]\n",
    "    )\n",
    "    \n",
    "    cos_loss = 1 - F.cosine_similarity(\n",
    "        hidden_states[is_reasoning_token],\n",
    "        target_embeddings[is_reasoning_token],\n",
    "        dim=-1\n",
    "    ).mean()\n",
    "\n",
    "    clipped_mse = min(mse_loss, 2)\n",
    "    embed_loss = 0.5 * clipped_mse + 1 * cos_loss\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = ce_loss + embed_loss\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'ce_loss': ce_loss,\n",
    "        'mse_loss': mse_loss,\n",
    "        'cos_loss': cos_loss\n",
    "    }\n",
    "\n",
    "def extract_boxed_answer(text):\n",
    "    \"\"\"Extract answer from \\boxed{} format\"\"\"\n",
    "    match = re.search(r'boxed{(.*?)}', text)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=1024):\n",
    "    dataset = load_dataset(\"ant-des/filtered_reasoning_deepseek\", split='train')\n",
    "    \n",
    "    def process_example(example):\n",
    "        prompt = example['messages'][0]['content']\n",
    "        answer = example['answer']\n",
    "        \n",
    "        # Tokenize prompt and answer separately\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)  # Include BOS token\n",
    "        answer_tokens = tokenizer(answer, add_special_tokens=False)  # Don't add extra tokens\n",
    "        \n",
    "        # Combine and truncate if needed\n",
    "        combined_input_ids = prompt_tokens['input_ids'] + answer_tokens['input_ids']\n",
    "        if len(combined_input_ids) > max_length:\n",
    "            combined_input_ids = combined_input_ids[:max_length]\n",
    "        \n",
    "        # Pad to max_length\n",
    "        padding_length = max_length - len(combined_input_ids)\n",
    "        input_ids = combined_input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(combined_input_ids) + [0] * padding_length\n",
    "        \n",
    "        # Create prompt mask (0 for prompt, 1 for answer)\n",
    "        prompt_mask = [0] * len(prompt_tokens['input_ids']) + [1] * (len(answer_tokens['input_ids']))\n",
    "        prompt_mask = prompt_mask + [0] * padding_length  # Add zeros for padding\n",
    "        \n",
    "        # Create labels (-100 for prompt and padding)\n",
    "        labels = [-100] * len(prompt_tokens['input_ids'])  # Mask prompt\n",
    "        labels.extend(answer_tokens['input_ids'])  # Add answer tokens as labels\n",
    "        labels.extend([-100] * padding_length)  # Mask padding\n",
    "        \n",
    "        # Ensure all tensors are the same length\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        prompt_mask = prompt_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'prompt_mask': torch.tensor(prompt_mask),\n",
    "            'labels': torch.tensor(labels)\n",
    "        }\n",
    "\n",
    "    processed_dataset = dataset.map(\n",
    "        process_example,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'prompt_mask', 'labels'])\n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "def train_step(batch, model, step):\n",
    "    # Calculate alpha for progressive schedule\n",
    "    alpha = min(step / 1000, 1.0)  # Progressive schedule over 1000 steps\n",
    "    #alpha = 0.0\n",
    "\n",
    "    # Move batch to device\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, hidden_states, final_hidden, is_reasoning_token = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels'],\n",
    "        prompt_mask=batch['prompt_mask'],\n",
    "        alpha_schedule=alpha\n",
    "    )\n",
    "    # Calculate loss\n",
    "    loss_dict = LatentThoughtLoss(\n",
    "        outputs=outputs,\n",
    "        hidden_states=hidden_states,\n",
    "        target_embeddings=final_hidden,\n",
    "        is_reasoning_token=is_reasoning_token\n",
    "    )\n",
    "\n",
    "    # Backward pass\n",
    "    loss_dict['total_loss'].backward()\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        'train/loss': loss_dict['total_loss'].item(),\n",
    "        'train/ce_loss': loss_dict['ce_loss'].item(),\n",
    "        'train/mse_loss': loss_dict['mse_loss'].item(),\n",
    "        'train/cos_loss': loss_dict['cos_loss'].item(),\n",
    "        'train/alpha': alpha,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"latent-thought-cot\", name=\"first-experiment\")\n",
    "    \n",
    "    model = LatentThoughtLLM(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "    \n",
    "    # Optimizer setup\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        betas=(0.9, 0.95)\n",
    "    )\n",
    "    \n",
    "    # Load dataset\n",
    "    train_dataset = prepare_dataset(model.tokenizer, max_length=1024)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    global_step = 0\n",
    "    max_grad_norm = 1.0\n",
    "    gradient_accumulation_steps = 8 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        # Training loop with tqdm\n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "           \n",
    "            # Training step\n",
    "            metrics = train_step(batch, model, global_step)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{metrics['train/loss']:.4f}\",\n",
    "                'ce_loss': f\"{metrics['train/ce_loss']:.4f}\",\n",
    "                'mse_loss': f\"{metrics['train/mse_loss']:.4f}\",\n",
    "                'cos_loss': f\"{metrics['train/cos_loss']:.4f}\",\n",
    "                'alpha': f\"{metrics['train/alpha']:.2f}\"\n",
    "            })\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # Debug prints every 100 steps\n",
    "            if global_step % 50 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Move sample batch to device\n",
    "                    sample_batch = {k: v[:1].to(model.device) for k, v in batch.items()}\n",
    "\n",
    "                    # Get non-padding tokens (move to same device as output_tokens)\n",
    "                    valid_tokens = sample_batch['attention_mask'][0].bool().to(model.device)\n",
    "                    \n",
    "                    print(\"\\nOriginal prompt:\")\n",
    "                    print(model.tokenizer.decode(sample_batch['input_ids'][0][valid_tokens]))\n",
    "                    \n",
    "                    sample_output = model(\n",
    "                        input_ids=sample_batch['input_ids'],\n",
    "                        attention_mask=sample_batch['attention_mask'],\n",
    "                        prompt_mask=sample_batch['prompt_mask'],\n",
    "                        alpha_schedule=metrics['train/alpha']\n",
    "                    )\n",
    "                    \n",
    "                    # Decode and print sample (only non-padding tokens)\n",
    "                    output_tokens = sample_output[0].logits[0].argmax(dim=-1)\n",
    "                    decoded = model.tokenizer.decode(\n",
    "                        output_tokens[valid_tokens].cpu(),\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    print(\"\\nSample output:\")\n",
    "                    print(decoded)\n",
    "                    print(\"\\nExtracted answer:\", extract_boxed_answer(decoded))\n",
    "                \n",
    "                model.train()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
